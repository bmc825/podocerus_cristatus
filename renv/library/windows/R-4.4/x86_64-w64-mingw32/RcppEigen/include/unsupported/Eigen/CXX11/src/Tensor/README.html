
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../../../../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Eigen Tensors {#eigen_tensors} &#8212; Chapter 3 Data Analysis</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../../../../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../../../../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../../../../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../../../../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../../../../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../../../../../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../../../../../../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../../../../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../../../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../../../../../../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../../../../../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../../../../../../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../../../../../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../../../../../../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'renv/library/windows/R-4.4/x86_64-w64-mingw32/RcppEigen/include/unsupported/Eigen/CXX11/src/Tensor/README';</script>
    <link rel="index" title="Index" href="../../../../../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../../../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../../../../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../../../../../../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../../../../../../../../../_static/Podocerus_bw.png" class="logo__image only-light" alt="Chapter 3 Data Analysis - Home"/>
    <img src="../../../../../../../../../../../../_static/Podocerus_bw.png" class="logo__image only-dark pst-js-only" alt="Chapter 3 Data Analysis - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../../../../../../../../../intro.html">
                    Data Analysis of P. cristatus
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../../../../scripts/Load_Packages.html">Load Packages</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../../../../../../../01_Population/01_intro.html">Question 1</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../01_Population/01_data_cleaning.html">Data Cleaning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../01_Population/Clustering.html">Cluster Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../01_Population/Population_structure.html">Population Structure Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../../../../../../../02_Occurrence/02_intro.html">Question 2</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../02_Occurrence/02_data_cleaning.html">Data Cleaning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../02_Occurrence/Model_1.html">Model 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../02_Occurrence/Model_2.html">Model 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../02_Occurrence/Model_3.html">Model 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../02_Occurrence/Model_exp.html">Substrate Experiment Model</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../../../../../../../03_Camouflage/03_intro.html">Question 3</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../03_Camouflage/03_data_cleaning.html">Data Cleaning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../03_Camouflage/Model_4.html">Model 4</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../03_Camouflage/Model_5.html">Model 5</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../03_Camouflage/Model_6.html">Model 6</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../../../../../../../04_Phylogenetic/04_intro.html">Question 4</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../04_Phylogenetic/Geneious_protocol.html">Geneious Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../../../../../../04_Phylogenetic/Phylogenetic_analysis.html">Phylogenetic Analysis</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../../../../../../../../../_sources/renv/library/windows/R-4.4/x86_64-w64-mingw32/RcppEigen/include/unsupported/Eigen/CXX11/src/Tensor/README.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Eigen Tensors {#eigen_tensors}</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-classes">Tensor Classes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensor-data-type-rank">Class Tensor&lt;data_type, rank&gt;</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor-tensor-data-type-rank-size0-size1">Constructor Tensor&lt;data_type, rank&gt;(size0, size1, …)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor-tensor-data-type-rank-size-array">Constructor Tensor&lt;data_type, rank&gt;(size_array)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensorfixedsize-data-type-sizes-size0-size1">Class TensorFixedSize&lt;data_type, Sizes&lt;size0, size1, …&gt;&gt;</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensormap-tensor-data-type-rank">Class TensorMap&lt;Tensor&lt;data_type, rank&gt;&gt;</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor-tensormap-tensor-data-type-rank-data-size0-size1">Constructor TensorMap&lt;Tensor&lt;data_type, rank&gt;&gt;(data, size0, size1, …)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensorref">Class TensorRef</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-tensor-elements">Accessing Tensor Elements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-tensor-index0-index1">&lt;data_type&gt; tensor(index0, index1…)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorlayout">TensorLayout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-operations">Tensor Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-operations-and-c-auto">Tensor Operations and C++ “auto”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-when-expression-are-evaluated">Controlling When Expression are Evaluated</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#assigning-to-a-tensor-tensorfixedsize-or-tensormap">Assigning to a Tensor, TensorFixedSize, or TensorMap.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-eval">Calling eval().</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#assigning-to-a-tensorref">Assigning to a TensorRef.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-how-expressions-are-evaluated">Controlling How Expressions Are Evaluated</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-with-the-defaultdevice">Evaluating With the DefaultDevice</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-with-a-thread-pool">Evaluating with a Thread Pool</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-on-gpu">Evaluating On GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datatypes">Datatypes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions"><tensor-type>::Dimensions</tensor-type></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#index"><tensor-type>::Index</tensor-type></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar"><tensor-type>::Scalar</tensor-type></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><operation></operation></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#built-in-tensor-methods">Built-in Tensor Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metadata">Metadata</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#int-numdimensions">int NumDimensions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions-dimensions">Dimensions dimensions()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-dimension-index-n">Index dimension(Index n)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-size">Index size()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-dimensions-from-an-operation">Getting Dimensions From An Operation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constructors">Constructors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorfixedsize">TensorFixedSize</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensormap">TensorMap</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents-initialization">Contents Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setconstant-const-scalar-val"><tensor-type> setConstant(const Scalar&amp; val)</tensor-type></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setzero"><tensor-type> setZero()</tensor-type></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setvalues-initializer-list"><tensor-type> setValues({..initializer_list})</tensor-type></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setrandom"><tensor-type> setRandom()</tensor-type></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-access">Data Access</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-data-and-const-scalar-data-const">Scalar* data() and const Scalar* data() const</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Tensor Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constant-const-scalar-val"><operation> constant(const Scalar&amp; val)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random"><operation> random()</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unary-element-wise-operations">Unary Element Wise Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator"><operation> operator-()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sqrt"><operation> sqrt()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rsqrt"><operation> rsqrt()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#square"><operation> square()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse"><operation> inverse()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exp"><operation> exp()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log"><operation> log()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#abs"><operation> abs()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pow-scalar-exponent"><operation> pow(Scalar exponent)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-scalar-scale"><operation>  operator * (Scalar scale)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemax-scalar-threshold"><operation>  cwiseMax(Scalar threshold)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemin-scalar-threshold"><operation>  cwiseMin(Scalar threshold)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unaryexpr-const-customunaryop-func"><operation>  unaryExpr(const CustomUnaryOp&amp; func)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-element-wise-operations">Binary Element Wise Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-const-otherderived-other"><operation> operator+(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><operation> operator-(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><operation> operator*(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><operation> operator/(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemax-const-otherderived-other"><operation> cwiseMax(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemin-const-otherderived-other"><operation> cwiseMin(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-operators"><operation> Logical operators</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-select-const-thenderived-thentensor-const-elsederived-elsetensor">Selection (select(const ThenDerived&amp; thenTensor, const ElseDerived&amp; elseTensor)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contraction">Contraction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-operations">Reduction Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-dimensions">Reduction Dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-along-all-dimensions">Reduction along all dimensions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-const-dimensions-new-dims"><operation> sum(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum"><operation> sum()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-const-dimensions-new-dims"><operation> mean(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean"><operation> mean()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-const-dimensions-new-dims"><operation> maximum(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum"><operation> maximum()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-const-dimensions-new-dims"><operation> minimum(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum"><operation> minimum()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prod-const-dimensions-new-dims"><operation> prod(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prod"><operation> prod()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#all-const-dimensions-new-dims"><operation> all(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#all"><operation> all()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#any-const-dimensions-new-dims"><operation> any(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#any"><operation> any()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-const-dimensions-new-dims-const-reducer-reducer"><operation> reduce(const Dimensions&amp; new_dims, const Reducer&amp; reducer)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trace">Trace</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trace-const-dimensions-new-dims"><operation> trace(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><operation> trace()</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scan-operations">Scan Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumsum-const-index-axis"><operation> cumsum(const Index&amp; axis)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumprod-const-index-axis"><operation> cumprod(const Index&amp; axis)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutions">Convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolve-const-kernel-kernel-const-dimensions-dims"><operation> convolve(const Kernel&amp; kernel, const Dimensions&amp; dims)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-operations">Geometrical Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshape-const-dimensions-new-dims"><operation> reshape(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffle-const-shuffle-shuffle"><operation> shuffle(const Shuffle&amp; shuffle)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride-const-strides-strides"><operation> stride(const Strides&amp; strides)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slice-const-startindices-offsets-const-sizes-extents"><operation> slice(const StartIndices&amp; offsets, const Sizes&amp; extents)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chip-const-index-offset-const-index-dim"><operation> chip(const Index offset, const Index dim)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-const-reversedimensions-reverse"><operation> reverse(const ReverseDimensions&amp; reverse)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcast-const-broadcast-broadcast"><operation> broadcast(const Broadcast&amp; broadcast)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#concatenate-const-otherderived-other-axis-axis"><operation> concatenate(const OtherDerived&amp; other, Axis axis)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pad-const-paddingdimensions-padding"><operation>  pad(const PaddingDimensions&amp; padding)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-patches-const-patchdims-patch-dims"><operation>  extract_patches(const PatchDims&amp; patch_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-image-patches-const-index-patch-rows-const-index-patch-cols-const-index-row-stride-const-index-col-stride-const-paddingtype-padding-type"><operation>  extract_image_patches(const Index patch_rows, const Index patch_cols, const Index row_stride, const Index col_stride, const PaddingType padding_type)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-operations">Special Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cast"><operation> cast<t>()</t></operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eval"><operation>     eval()</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representation-of-scalar-values">Representation of scalar values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="eigen-tensors-eigen-tensors">
<h1>Eigen Tensors {#eigen_tensors}<a class="headerlink" href="#eigen-tensors-eigen-tensors" title="Link to this heading">#</a></h1>
<p>Tensors are multidimensional arrays of elements. Elements are typically scalars,
but more complex types such as strings are also supported.</p>
<section id="tensor-classes">
<h2>Tensor Classes<a class="headerlink" href="#tensor-classes" title="Link to this heading">#</a></h2>
<p>You can manipulate a tensor with one of the following classes.  They all are in
the namespace <code class="docutils literal notranslate"><span class="pre">::Eigen.</span></code></p>
<section id="class-tensor-data-type-rank">
<h3>Class Tensor&lt;data_type, rank&gt;<a class="headerlink" href="#class-tensor-data-type-rank" title="Link to this heading">#</a></h3>
<p>This is the class to use to create a tensor and allocate memory for it.  The
class is templatized with the tensor datatype, such as float or int, and the
tensor rank.  The rank is the number of dimensions, for example rank 2 is a
matrix.</p>
<p>Tensors of this class are resizable.  For example, if you assign a tensor of a
different size to a Tensor, that tensor is resized to match its new value.</p>
<section id="constructor-tensor-data-type-rank-size0-size1">
<h4>Constructor Tensor&lt;data_type, rank&gt;(size0, size1, …)<a class="headerlink" href="#constructor-tensor-data-type-rank-size0-size1" title="Link to this heading">#</a></h4>
<p>Constructor for a Tensor.  The constructor must be passed <code class="docutils literal notranslate"><span class="pre">rank</span></code> integers
indicating the sizes of the instance along each of the the <code class="docutils literal notranslate"><span class="pre">rank</span></code>
dimensions.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a tensor of rank 3 of sizes 2, 3, 4.  This tensor owns
// memory to hold 24 floating point values (24 = 2 x 3 x 4).
Tensor&lt;float, 3&gt; t_3d(2, 3, 4);

// Resize t_3d by assigning a tensor of different sizes, but same rank.
t_3d = Tensor&lt;float, 3&gt;(3, 4, 3);
</pre></div>
</div>
</section>
<section id="constructor-tensor-data-type-rank-size-array">
<h4>Constructor Tensor&lt;data_type, rank&gt;(size_array)<a class="headerlink" href="#constructor-tensor-data-type-rank-size-array" title="Link to this heading">#</a></h4>
<p>Constructor where the sizes for the constructor are specified as an array of
values instead of an explicitly list of parameters.  The array type to use is
<code class="docutils literal notranslate"><span class="pre">Eigen::array&lt;Eigen::Index&gt;</span></code>.  The array can be constructed automatically
from an initializer list.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a tensor of strings of rank 2 with sizes 5, 7.
Tensor&lt;string, 2&gt; t_2d({5, 7});
</pre></div>
</div>
</section>
</section>
<section id="class-tensorfixedsize-data-type-sizes-size0-size1">
<h3>Class TensorFixedSize&lt;data_type, Sizes&lt;size0, size1, …&gt;&gt;<a class="headerlink" href="#class-tensorfixedsize-data-type-sizes-size0-size1" title="Link to this heading">#</a></h3>
<p>Class to use for tensors of fixed size, where the size is known at compile
time.  Fixed sized tensors can provide very fast computations because all their
dimensions are known by the compiler.  FixedSize tensors are not resizable.</p>
<p>If the total number of elements in a fixed size tensor is small enough the
tensor data is held onto the stack and does not cause heap allocation and free.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a 4 x 3 tensor of floats.
TensorFixedSize&lt;float, Sizes&lt;4, 3&gt;&gt; t_4x3;
</pre></div>
</div>
</section>
<section id="class-tensormap-tensor-data-type-rank">
<h3>Class TensorMap&lt;Tensor&lt;data_type, rank&gt;&gt;<a class="headerlink" href="#class-tensormap-tensor-data-type-rank" title="Link to this heading">#</a></h3>
<p>This is the class to use to create a tensor on top of memory allocated and
owned by another part of your code.  It allows to view any piece of allocated
memory as a Tensor.  Instances of this class do not own the memory where the
data are stored.</p>
<p>A TensorMap is not resizable because it does not own the memory where its data
are stored.</p>
<section id="constructor-tensormap-tensor-data-type-rank-data-size0-size1">
<h4>Constructor TensorMap&lt;Tensor&lt;data_type, rank&gt;&gt;(data, size0, size1, …)<a class="headerlink" href="#constructor-tensormap-tensor-data-type-rank-data-size0-size1" title="Link to this heading">#</a></h4>
<p>Constructor for a Tensor.  The constructor must be passed a pointer to the
storage for the data, and “rank” size attributes.  The storage has to be
large enough to hold all the data.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Map a tensor of ints on top of stack-allocated storage.
int storage[128];  // 2 x 4 x 2 x 8 = 128
TensorMap&lt;Tensor&lt;int, 4&gt;&gt; t_4d(storage, 2, 4, 2, 8);

// The same storage can be viewed as a different tensor.
// You can also pass the sizes as an array.
TensorMap&lt;Tensor&lt;int, 2&gt;&gt; t_2d(storage, 16, 8);

// You can also map fixed-size tensors.  Here we get a 1d view of
// the 2d fixed-size tensor.
TensorFixedSize&lt;float, Sizes&lt;4, 3&gt;&gt; t_4x3;
TensorMap&lt;Tensor&lt;float, 1&gt;&gt; t_12(t_4x3.data(), 12);
</pre></div>
</div>
</section>
<section id="class-tensorref">
<h4>Class TensorRef<a class="headerlink" href="#class-tensorref" title="Link to this heading">#</a></h4>
<p>See Assigning to a TensorRef below.</p>
</section>
</section>
</section>
<section id="accessing-tensor-elements">
<h2>Accessing Tensor Elements<a class="headerlink" href="#accessing-tensor-elements" title="Link to this heading">#</a></h2>
<section id="data-type-tensor-index0-index1">
<h3>&lt;data_type&gt; tensor(index0, index1…)<a class="headerlink" href="#data-type-tensor-index0-index1" title="Link to this heading">#</a></h3>
<p>Return the element at position <code class="docutils literal notranslate"><span class="pre">(index0,</span> <span class="pre">index1...)</span></code> in tensor
<code class="docutils literal notranslate"><span class="pre">tensor</span></code>.  You must pass as many parameters as the rank of <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.
The expression can be used as an l-value to set the value of the element at the
specified position.  The value returned is of the datatype of the tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Set the value of the element at position (0, 1, 0);
Tensor&lt;float, 3&gt; t_3d(2, 3, 4);
t_3d(0, 1, 0) = 12.0f;

// Initialize all elements to random values.
for (int i = 0; i &lt; 2; ++i) {
  for (int j = 0; j &lt; 3; ++j) {
    for (int k = 0; k &lt; 4; ++k) {
      t_3d(i, j, k) = ...some random value...;
    }
  }
}

// Print elements of a tensor.
for (int i = 0; i &lt; 2; ++i) {
  LOG(INFO) &lt;&lt; t_3d(i, 0, 0);
}
</pre></div>
</div>
</section>
</section>
<section id="tensorlayout">
<h2>TensorLayout<a class="headerlink" href="#tensorlayout" title="Link to this heading">#</a></h2>
<p>The tensor library supports 2 layouts: <code class="docutils literal notranslate"><span class="pre">ColMajor</span></code> (the default) and
<code class="docutils literal notranslate"><span class="pre">RowMajor</span></code>.  Only the default column major layout is currently fully
supported, and it is therefore not recommended to attempt to use the row major
layout at the moment.</p>
<p>The layout of a tensor is optionally specified as part of its type. If not
specified explicitly column major is assumed.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 3, ColMajor&gt; col_major;  // equivalent to Tensor&lt;float, 3&gt;
TensorMap&lt;Tensor&lt;float, 3, RowMajor&gt; &gt; row_major(data, ...);
</pre></div>
</div>
<p>All the arguments to an expression must use the same layout. Attempting to mix
different layouts will result in a compilation error.</p>
<p>It is possible to change the layout of a tensor or an expression using the
<code class="docutils literal notranslate"><span class="pre">swap_layout()</span></code> method.  Note that this will also reverse the order of the
dimensions.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 2, ColMajor&gt; col_major(2, 4);
Tensor&lt;float, 2, RowMajor&gt; row_major(2, 4);

Tensor&lt;float, 2&gt; col_major_result = col_major;  // ok, layouts match
Tensor&lt;float, 2&gt; col_major_result = row_major;  // will not compile

// Simple layout swap
col_major_result = row_major.swap_layout();
eigen_assert(col_major_result.dimension(0) == 4);
eigen_assert(col_major_result.dimension(1) == 2);

// Swap the layout and preserve the order of the dimensions
array&lt;int, 2&gt; shuffle(1, 0);
col_major_result = row_major.swap_layout().shuffle(shuffle);
eigen_assert(col_major_result.dimension(0) == 2);
eigen_assert(col_major_result.dimension(1) == 4);
</pre></div>
</div>
</section>
<section id="tensor-operations">
<h2>Tensor Operations<a class="headerlink" href="#tensor-operations" title="Link to this heading">#</a></h2>
<p>The Eigen Tensor library provides a vast library of operations on Tensors:
numerical operations such as addition and multiplication, geometry operations
such as slicing and shuffling, etc.  These operations are available as methods
of the Tensor classes, and in some cases as operator overloads.  For example
the following code computes the elementwise addition of two tensors:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 3&gt; t1(2, 3, 4);
...set some values in t1...
Tensor&lt;float, 3&gt; t2(2, 3, 4);
...set some values in t2...
// Set t3 to the element wise sum of t1 and t2
Tensor&lt;float, 3&gt; t3 = t1 + t2;
</pre></div>
</div>
<p>While the code above looks easy enough, it is important to understand that the
expression <code class="docutils literal notranslate"><span class="pre">t1</span> <span class="pre">+</span> <span class="pre">t2</span></code> is not actually adding the values of the tensors.  The
expression instead constructs a “tensor operator” object of the class
TensorCwiseBinaryOp&lt;scalar_sum&gt;, which has references to the tensors
<code class="docutils literal notranslate"><span class="pre">t1</span></code> and <code class="docutils literal notranslate"><span class="pre">t2</span></code>.  This is a small C++ object that knows how to add
<code class="docutils literal notranslate"><span class="pre">t1</span></code> and <code class="docutils literal notranslate"><span class="pre">t2</span></code>.  It is only when the value of the expression is assigned
to the tensor <code class="docutils literal notranslate"><span class="pre">t3</span></code> that the addition is actually performed.  Technically,
this happens through the overloading of <code class="docutils literal notranslate"><span class="pre">operator=()</span></code> in the Tensor class.</p>
<p>This mechanism for computing tensor expressions allows for lazy evaluation and
optimizations which are what make the tensor library very fast.</p>
<p>Of course, the tensor operators do nest, and the expression <code class="docutils literal notranslate"><span class="pre">t1</span> <span class="pre">+</span> <span class="pre">t2</span> <span class="pre">*</span> <span class="pre">0.3f</span></code>
is actually represented with the (approximate) tree of operators:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>TensorCwiseBinaryOp&lt;scalar_sum&gt;(t1, TensorCwiseUnaryOp&lt;scalar_mul&gt;(t2, 0.3f))
</pre></div>
</div>
<section id="tensor-operations-and-c-auto">
<h3>Tensor Operations and C++ “auto”<a class="headerlink" href="#tensor-operations-and-c-auto" title="Link to this heading">#</a></h3>
<p>Because Tensor operations create tensor operators, the C++ <code class="docutils literal notranslate"><span class="pre">auto</span></code> keyword
does not have its intuitive meaning.  Consider these 2 lines of code:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 3&gt; t3 = t1 + t2;
auto t4 = t1 + t2;
</pre></div>
</div>
<p>In the first line we allocate the tensor <code class="docutils literal notranslate"><span class="pre">t3</span></code> and it will contain the
result of the addition of <code class="docutils literal notranslate"><span class="pre">t1</span></code> and <code class="docutils literal notranslate"><span class="pre">t2</span></code>.  In the second line, <code class="docutils literal notranslate"><span class="pre">t4</span></code>
is actually the tree of tensor operators that will compute the addition of
<code class="docutils literal notranslate"><span class="pre">t1</span></code> and <code class="docutils literal notranslate"><span class="pre">t2</span></code>.  In fact, <code class="docutils literal notranslate"><span class="pre">t4</span></code> is <em>not</em> a tensor and you cannot get
the values of its elements:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 3&gt; t3 = t1 + t2;
cout &lt;&lt; t3(0, 0, 0);  // OK prints the value of t1(0, 0, 0) + t2(0, 0, 0)

auto t4 = t1 + t2;
cout &lt;&lt; t4(0, 0, 0);  // Compilation error!
</pre></div>
</div>
<p>When you use <code class="docutils literal notranslate"><span class="pre">auto</span></code> you do not get a Tensor as a result but instead a
non-evaluated expression.  So only use <code class="docutils literal notranslate"><span class="pre">auto</span></code> to delay evaluation.</p>
<p>Unfortunately, there is no single underlying concrete type for holding
non-evaluated expressions, hence you have to use auto in the case when you do
want to hold non-evaluated expressions.</p>
<p>When you need the results of set of tensor computations you have to assign the
result to a Tensor that will be capable of holding onto them.  This can be
either a normal Tensor, a fixed size Tensor, or a TensorMap on an existing
piece of memory.  All the following will work:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>auto t4 = t1 + t2;

Tensor&lt;float, 3&gt; result = t4;  // Could also be: result(t4);
cout &lt;&lt; result(0, 0, 0);

TensorMap&lt;float, 4&gt; result(&lt;a float* with enough space&gt;, &lt;size0&gt;, ...) = t4;
cout &lt;&lt; result(0, 0, 0);

TensorFixedSize&lt;float, Sizes&lt;size0, ...&gt;&gt; result = t4;
cout &lt;&lt; result(0, 0, 0);
</pre></div>
</div>
<p>Until you need the results, you can keep the operation around, and even reuse
it for additional operations.  As long as you keep the expression as an
operation, no computation is performed.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// One way to compute exp((t1 + t2) * 0.2f);
auto t3 = t1 + t2;
auto t4 = t3 * 0.2f;
auto t5 = t4.exp();
Tensor&lt;float, 3&gt; result = t5;

// Another way, exactly as efficient as the previous one:
Tensor&lt;float, 3&gt; result = ((t1 + t2) * 0.2f).exp();
</pre></div>
</div>
</section>
<section id="controlling-when-expression-are-evaluated">
<h3>Controlling When Expression are Evaluated<a class="headerlink" href="#controlling-when-expression-are-evaluated" title="Link to this heading">#</a></h3>
<p>There are several ways to control when expressions are evaluated:</p>
<ul class="simple">
<li><p>Assignment to a Tensor, TensorFixedSize, or TensorMap.</p></li>
<li><p>Use of the eval() method.</p></li>
<li><p>Assignment to a TensorRef.</p></li>
</ul>
<section id="assigning-to-a-tensor-tensorfixedsize-or-tensormap">
<h4>Assigning to a Tensor, TensorFixedSize, or TensorMap.<a class="headerlink" href="#assigning-to-a-tensor-tensorfixedsize-or-tensormap" title="Link to this heading">#</a></h4>
<p>The most common way to evaluate an expression is to assign it to a Tensor.  In
the example below, the <code class="docutils literal notranslate"><span class="pre">auto</span></code> declarations make the intermediate values
“Operations”, not Tensors, and do not cause the expressions to be evaluated.
The assignment to the Tensor <code class="docutils literal notranslate"><span class="pre">result</span></code> causes the evaluation of all the
operations.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>auto t3 = t1 + t2;             // t3 is an Operation.
auto t4 = t3 * 0.2f;           // t4 is an Operation.
auto t5 = t4.exp();            // t5 is an Operation.
Tensor&lt;float, 3&gt; result = t5;  // The operations are evaluated.
</pre></div>
</div>
<p>If you know the ranks and sizes of the Operation value you can assign the
Operation to a TensorFixedSize instead of a Tensor, which is a bit more
efficient.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// We know that the result is a 4x4x2 tensor!
TensorFixedSize&lt;float, Sizes&lt;4, 4, 2&gt;&gt; result = t5;
</pre></div>
</div>
<p>Simiarly, assigning an expression to a TensorMap causes its evaluation.  Like
tensors of type TensorFixedSize, TensorMaps cannot be resized so they have to
have the rank and sizes of the expression that are assigned to them.</p>
</section>
<section id="calling-eval">
<h4>Calling eval().<a class="headerlink" href="#calling-eval" title="Link to this heading">#</a></h4>
<p>When you compute large composite expressions, you sometimes want to tell Eigen
that an intermediate value in the expression tree is worth evaluating ahead of
time.  This is done by inserting a call to the <code class="docutils literal notranslate"><span class="pre">eval()</span></code> method of the
expression Operation.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// The previous example could have been written:
Tensor&lt;float, 3&gt; result = ((t1 + t2) * 0.2f).exp();

// If you want to compute (t1 + t2) once ahead of time you can write:
Tensor&lt;float, 3&gt; result = ((t1 + t2).eval() * 0.2f).exp();
</pre></div>
</div>
<p>Semantically, calling <code class="docutils literal notranslate"><span class="pre">eval()</span></code> is equivalent to materializing the value of
the expression in a temporary Tensor of the right size.  The code above in
effect does:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// .eval() knows the size!
TensorFixedSize&lt;float, Sizes&lt;4, 4, 2&gt;&gt; tmp = t1 + t2;
Tensor&lt;float, 3&gt; result = (tmp * 0.2f).exp();
</pre></div>
</div>
<p>Note that the return value of <code class="docutils literal notranslate"><span class="pre">eval()</span></code> is itself an Operation, so the
following code does not do what you may think:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Here t3 is an evaluation Operation.  t3 has not been evaluated yet.
auto t3 = (t1 + t2).eval();

// You can use t3 in another expression.  Still no evaluation.
auto t4 = (t3 * 0.2f).exp();

// The value is evaluated when you assign the Operation to a Tensor, using
// an intermediate tensor to represent t3.x
Tensor&lt;float, 3&gt; result = t4;
</pre></div>
</div>
<p>While in the examples above calling <code class="docutils literal notranslate"><span class="pre">eval()</span></code> does not make a difference in
performance, in other cases it can make a huge difference.  In the expression
below the <code class="docutils literal notranslate"><span class="pre">broadcast()</span></code> expression causes the <code class="docutils literal notranslate"><span class="pre">X.maximum()</span></code> expression
to be evaluated many times:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;...&gt; X ...;
Tensor&lt;...&gt; Y = ((X - X.maximum(depth_dim).reshape(dims2d).broadcast(bcast))
                 * beta).exp();
</pre></div>
</div>
<p>Inserting a call to <code class="docutils literal notranslate"><span class="pre">eval()</span></code> between the <code class="docutils literal notranslate"><span class="pre">maximum()</span></code> and
<code class="docutils literal notranslate"><span class="pre">reshape()</span></code> calls guarantees that maximum() is only computed once and
greatly speeds-up execution:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;...&gt; Y =
  ((X - X.maximum(depth_dim).eval().reshape(dims2d).broadcast(bcast))
    * beta).exp();
</pre></div>
</div>
<p>In the other example below, the tensor <code class="docutils literal notranslate"><span class="pre">Y</span></code> is both used in the expression
and its assignment.  This is an aliasing problem and if the evaluation is not
done in the right order Y will be updated incrementally during the evaluation
resulting in bogus results:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Tensor&lt;...&gt; Y ...;
 Y = Y / (Y.sum(depth_dim).reshape(dims2d).broadcast(bcast));
</pre></div>
</div>
<p>Inserting a call to <code class="docutils literal notranslate"><span class="pre">eval()</span></code> between the <code class="docutils literal notranslate"><span class="pre">sum()</span></code> and <code class="docutils literal notranslate"><span class="pre">reshape()</span></code>
expressions ensures that the sum is computed before any updates to <code class="docutils literal notranslate"><span class="pre">Y</span></code> are
done.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Y = Y / (Y.sum(depth_dim).eval().reshape(dims2d).broadcast(bcast));
</pre></div>
</div>
<p>Note that an eval around the full right hand side expression is not needed
because the generated has to compute the i-th value of the right hand side
before assigning it to the left hand side.</p>
<p>However, if you were assigning the expression value to a shuffle of <code class="docutils literal notranslate"><span class="pre">Y</span></code>
then you would need to force an eval for correctness by adding an <code class="docutils literal notranslate"><span class="pre">eval()</span></code>
call for the right hand side:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Y.shuffle(...) =
    (Y / (Y.sum(depth_dim).eval().reshape(dims2d).broadcast(bcast))).eval();
</pre></div>
</div>
</section>
<section id="assigning-to-a-tensorref">
<h4>Assigning to a TensorRef.<a class="headerlink" href="#assigning-to-a-tensorref" title="Link to this heading">#</a></h4>
<p>If you need to access only a few elements from the value of an expression you
can avoid materializing the value in a full tensor by using a TensorRef.</p>
<p>A TensorRef is a small wrapper class for any Eigen Operation.  It provides
overloads for the <code class="docutils literal notranslate"><span class="pre">()</span></code> operator that let you access individual values in
the expression.  TensorRef is convenient, because the Operation themselves do
not provide a way to access individual elements.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a TensorRef for the expression.  The expression is not
// evaluated yet.
TensorRef&lt;Tensor&lt;float, 3&gt; &gt; ref = ((t1 + t2) * 0.2f).exp();

// Use &quot;ref&quot; to access individual elements.  The expression is evaluated
// on the fly.
float at_0 = ref(0, 0, 0);
cout &lt;&lt; ref(0, 1, 0);
</pre></div>
</div>
<p>Only use TensorRef when you need a subset of the values of the expression.
TensorRef only computes the values you access.  However note that if you are
going to access all the values it will be much faster to materialize the
results in a Tensor first.</p>
<p>In some cases, if the full Tensor result would be very large, you may save
memory by accessing it as a TensorRef.  But not always.  So don’t count on it.</p>
</section>
</section>
<section id="controlling-how-expressions-are-evaluated">
<h3>Controlling How Expressions Are Evaluated<a class="headerlink" href="#controlling-how-expressions-are-evaluated" title="Link to this heading">#</a></h3>
<p>The tensor library provides several implementations of the various operations
such as contractions and convolutions.  The implementations are optimized for
different environments: single threaded on CPU, multi threaded on CPU, or on a
GPU using cuda.  Additional implementations may be added later.</p>
<p>You can choose which implementation to use with the <code class="docutils literal notranslate"><span class="pre">device()</span></code> call.  If
you do not choose an implementation explicitly the default implementation that
uses a single thread on the CPU is used.</p>
<p>The default implementation has been optimized for recent Intel CPUs, taking
advantage of SSE, AVX, and FMA instructions.  Work is ongoing to tune the
library on ARM CPUs.  Note that you need to pass compiler-dependent flags
to enable the use of SSE, AVX, and other instructions.</p>
<p>For example, the following code adds two tensors using the default
single-threaded CPU implementation:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 2&gt; a(30, 40);
Tensor&lt;float, 2&gt; b(30, 40);
Tensor&lt;float, 2&gt; c = a + b;
</pre></div>
</div>
<p>To choose a different implementation you have to insert a <code class="docutils literal notranslate"><span class="pre">device()</span></code> call
before the assignment of the result.  For technical C++ reasons this requires
that the Tensor for the result be declared on its own.  This means that you
have to know the size of the result.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; c(30, 40);
c.device(...) = a + b;
</pre></div>
</div>
<p>The call to <code class="docutils literal notranslate"><span class="pre">device()</span></code> must be the last call on the left of the operator=.</p>
<p>You must pass to the <code class="docutils literal notranslate"><span class="pre">device()</span></code> call an Eigen device object.  There are
presently three devices you can use: DefaultDevice, ThreadPoolDevice and
GpuDevice.</p>
<section id="evaluating-with-the-defaultdevice">
<h4>Evaluating With the DefaultDevice<a class="headerlink" href="#evaluating-with-the-defaultdevice" title="Link to this heading">#</a></h4>
<p>This is exactly the same as not inserting a <code class="docutils literal notranslate"><span class="pre">device()</span></code> call.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>DefaultDevice my_device;
c.device(my_device) = a + b;
</pre></div>
</div>
</section>
<section id="evaluating-with-a-thread-pool">
<h4>Evaluating with a Thread Pool<a class="headerlink" href="#evaluating-with-a-thread-pool" title="Link to this heading">#</a></h4>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create the Eigen ThreadPool
Eigen::ThreadPool pool(8 /* number of threads in pool */)

// Create the Eigen ThreadPoolDevice.
Eigen::ThreadPoolDevice my_device(&amp;pool, 4 /* number of threads to use */);

// Now just use the device when evaluating expressions.
Eigen::Tensor&lt;float, 2&gt; c(30, 50);
c.device(my_device) = a.contract(b, dot_product_dims);
</pre></div>
</div>
</section>
<section id="evaluating-on-gpu">
<h4>Evaluating On GPU<a class="headerlink" href="#evaluating-on-gpu" title="Link to this heading">#</a></h4>
<p>This is presently a bit more complicated than just using a thread pool device.
You need to create a GPU device but you also need to explicitly allocate the
memory for tensors with cuda.</p>
</section>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<section id="datatypes">
<h3>Datatypes<a class="headerlink" href="#datatypes" title="Link to this heading">#</a></h3>
<p>In the documentation of the tensor methods and Operation we mention datatypes
that are tensor-type specific:</p>
<section id="dimensions">
<h4><Tensor-Type>::Dimensions<a class="headerlink" href="#dimensions" title="Link to this heading">#</a></h4>
<p>Acts like an array of ints.  Has an <code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">size</span></code> attribute, and can be
indexed like an array to access individual values.  Used to represent the
dimensions of a tensor.  See <code class="docutils literal notranslate"><span class="pre">dimensions()</span></code>.</p>
</section>
<section id="index">
<h4><Tensor-Type>::Index<a class="headerlink" href="#index" title="Link to this heading">#</a></h4>
<p>Acts like an <code class="docutils literal notranslate"><span class="pre">int</span></code>.  Used for indexing tensors along their dimensions.  See
<code class="docutils literal notranslate"><span class="pre">operator()</span></code>, <code class="docutils literal notranslate"><span class="pre">dimension()</span></code>, and <code class="docutils literal notranslate"><span class="pre">size()</span></code>.</p>
</section>
<section id="scalar">
<h4><Tensor-Type>::Scalar<a class="headerlink" href="#scalar" title="Link to this heading">#</a></h4>
<p>Represents the datatype of individual tensor elements.  For example, for a
<code class="docutils literal notranslate"><span class="pre">Tensor&lt;float&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">Scalar</span></code> is the type <code class="docutils literal notranslate"><span class="pre">float</span></code>.  See
<code class="docutils literal notranslate"><span class="pre">setConstant()</span></code>.</p>
</section>
<section id="id1">
<h4><Operation><a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>We use this pseudo type to indicate that a tensor Operation is returned by a
method.  We indicate in the text the type and dimensions of the tensor that the
Operation returns after evaluation.</p>
<p>The Operation will have to be evaluated, for example by assigning it to a
tensor, before you can access the values of the resulting tensor.  You can also
access the values through a TensorRef.</p>
</section>
</section>
</section>
<section id="built-in-tensor-methods">
<h2>Built-in Tensor Methods<a class="headerlink" href="#built-in-tensor-methods" title="Link to this heading">#</a></h2>
<p>These are usual C++ methods that act on tensors immediately.  They are not
Operations which provide delayed evaluation of their results.  Unless specified
otherwise, all the methods listed below are available on all tensor classes:
Tensor, TensorFixedSize, and TensorMap.</p>
</section>
<section id="metadata">
<h2>Metadata<a class="headerlink" href="#metadata" title="Link to this heading">#</a></h2>
<section id="int-numdimensions">
<h3>int NumDimensions<a class="headerlink" href="#int-numdimensions" title="Link to this heading">#</a></h3>
<p>Constant value indicating the number of dimensions of a Tensor.  This is also
known as the tensor “rank”.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>  Eigen::Tensor&lt;float, 2&gt; a(3, 4);
  cout &lt;&lt; &quot;Dims &quot; &lt;&lt; a.NumDimensions;
  =&gt; Dims 2
</pre></div>
</div>
</section>
<section id="dimensions-dimensions">
<h3>Dimensions dimensions()<a class="headerlink" href="#dimensions-dimensions" title="Link to this heading">#</a></h3>
<p>Returns an array-like object representing the dimensions of the tensor.
The actual type of the <code class="docutils literal notranslate"><span class="pre">dimensions()</span></code> result is <code class="docutils literal notranslate"><span class="pre">&lt;Tensor-Type&gt;::``Dimensions</span></code>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(3, 4);
const Eigen::Tensor&lt;float, 2&gt;::Dimensions&amp; d = a.dimensions();
cout &lt;&lt; &quot;Dim size: &quot; &lt;&lt; d.size &lt;&lt; &quot;, dim 0: &quot; &lt;&lt; d[0]
     &lt;&lt; &quot;, dim 1: &quot; &lt;&lt; d[1];
=&gt; Dim size: 2, dim 0: 3, dim 1: 4
</pre></div>
</div>
<p>If you use a C++11 compiler, you can use <code class="docutils literal notranslate"><span class="pre">auto</span></code> to simplify the code:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>const auto&amp; d = a.dimensions();
cout &lt;&lt; &quot;Dim size: &quot; &lt;&lt; d.size &lt;&lt; &quot;, dim 0: &quot; &lt;&lt; d[0]
     &lt;&lt; &quot;, dim 1: &quot; &lt;&lt; d[1];
=&gt; Dim size: 2, dim 0: 3, dim 1: 4
</pre></div>
</div>
</section>
<section id="index-dimension-index-n">
<h3>Index dimension(Index n)<a class="headerlink" href="#index-dimension-index-n" title="Link to this heading">#</a></h3>
<p>Returns the n-th dimension of the tensor.  The actual type of the
<code class="docutils literal notranslate"><span class="pre">dimension()</span></code> result is <code class="docutils literal notranslate"><span class="pre">&lt;Tensor-Type&gt;::``Index</span></code>, but you can
always use it like an int.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>  Eigen::Tensor&lt;float, 2&gt; a(3, 4);
  int dim1 = a.dimension(1);
  cout &lt;&lt; &quot;Dim 1: &quot; &lt;&lt; dim1;
  =&gt; Dim 1: 4
</pre></div>
</div>
</section>
<section id="index-size">
<h3>Index size()<a class="headerlink" href="#index-size" title="Link to this heading">#</a></h3>
<p>Returns the total number of elements in the tensor.  This is the product of all
the tensor dimensions.  The actual type of the <code class="docutils literal notranslate"><span class="pre">size()</span></code> result is
<code class="docutils literal notranslate"><span class="pre">&lt;Tensor-Type&gt;::``Index</span></code>, but you can always use it like an int.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(3, 4);
cout &lt;&lt; &quot;Size: &quot; &lt;&lt; a.size();
=&gt; Size: 12
</pre></div>
</div>
</section>
<section id="getting-dimensions-from-an-operation">
<h3>Getting Dimensions From An Operation<a class="headerlink" href="#getting-dimensions-from-an-operation" title="Link to this heading">#</a></h3>
<p>A few operations provide <code class="docutils literal notranslate"><span class="pre">dimensions()</span></code> directly,
e.g. <code class="docutils literal notranslate"><span class="pre">TensorReslicingOp</span></code>.  Most operations defer calculating dimensions
until the operation is being evaluated.  If you need access to the dimensions
of a deferred operation, you can wrap it in a TensorRef (see Assigning to a
TensorRef above), which provides <code class="docutils literal notranslate"><span class="pre">dimensions()</span></code> and <code class="docutils literal notranslate"><span class="pre">dimension()</span></code> as
above.</p>
<p>TensorRef can also wrap the plain Tensor types, so this is a useful idiom in
templated contexts where the underlying object could be either a raw Tensor
or some deferred operation (e.g. a slice of a Tensor).  In this case, the
template code can wrap the object in a TensorRef and reason about its
dimensionality while remaining agnostic to the underlying type.</p>
</section>
</section>
<section id="constructors">
<h2>Constructors<a class="headerlink" href="#constructors" title="Link to this heading">#</a></h2>
<section id="tensor">
<h3>Tensor<a class="headerlink" href="#tensor" title="Link to this heading">#</a></h3>
<p>Creates a tensor of the specified size. The number of arguments must be equal
to the rank of the tensor. The content of the tensor is not initialized.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(3, 4);
cout &lt;&lt; &quot;NumRows: &quot; &lt;&lt; a.dimension(0) &lt;&lt; &quot; NumCols: &quot; &lt;&lt; a.dimension(1) &lt;&lt; endl;
=&gt; NumRows: 3 NumCols: 4
</pre></div>
</div>
</section>
<section id="tensorfixedsize">
<h3>TensorFixedSize<a class="headerlink" href="#tensorfixedsize" title="Link to this heading">#</a></h3>
<p>Creates a tensor of the specified size. The number of arguments in the Sizes&lt;&gt;
template parameter determines the rank of the tensor. The content of the tensor
is not initialized.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::TensorFixedSize&lt;float, Sizes&lt;3, 4&gt;&gt; a;
cout &lt;&lt; &quot;Rank: &quot; &lt;&lt; a.rank() &lt;&lt; endl;
=&gt; Rank: 2
cout &lt;&lt; &quot;NumRows: &quot; &lt;&lt; a.dimension(0) &lt;&lt; &quot; NumCols: &quot; &lt;&lt; a.dimension(1) &lt;&lt; endl;
=&gt; NumRows: 3 NumCols: 4
</pre></div>
</div>
</section>
<section id="tensormap">
<h3>TensorMap<a class="headerlink" href="#tensormap" title="Link to this heading">#</a></h3>
<p>Creates a tensor mapping an existing array of data. The data must not be freed
until the TensorMap is discarded, and the size of the data must be large enough
to accommodate the coefficients of the tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>float data[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11};
Eigen::TensorMap&lt;Tensor&lt;float, 2&gt;&gt; a(data, 3, 4);
cout &lt;&lt; &quot;NumRows: &quot; &lt;&lt; a.dimension(0) &lt;&lt; &quot; NumCols: &quot; &lt;&lt; a.dimension(1) &lt;&lt; endl;
=&gt; NumRows: 3 NumCols: 4
cout &lt;&lt; &quot;a(1, 2): &quot; &lt;&lt; a(1, 2) &lt;&lt; endl;
=&gt; a(1, 2): 7
</pre></div>
</div>
</section>
</section>
<section id="contents-initialization">
<h2>Contents Initialization<a class="headerlink" href="#contents-initialization" title="Link to this heading">#</a></h2>
<p>When a new Tensor or a new TensorFixedSize are created, memory is allocated to
hold all the tensor elements, but the memory is not initialized.  Similarly,
when a new TensorMap is created on top of non-initialized memory the memory its
contents are not initialized.</p>
<p>You can use one of the methods below to initialize the tensor memory.  These
have an immediate effect on the tensor and return the tensor itself as a
result.  These are not tensor Operations which delay evaluation.</p>
<section id="setconstant-const-scalar-val">
<h3><Tensor-Type> setConstant(const Scalar&amp; val)<a class="headerlink" href="#setconstant-const-scalar-val" title="Link to this heading">#</a></h3>
<p>Sets all elements of the tensor to the constant value <code class="docutils literal notranslate"><span class="pre">val</span></code>.  <code class="docutils literal notranslate"><span class="pre">Scalar</span></code>
is the type of data stored in the tensor.  You can pass any value that is
convertible to that type.</p>
<p>Returns the tensor itself in case you want to chain another call.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a.setConstant(12.3f);
cout &lt;&lt; &quot;Constant: &quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
=&gt;
Constant:
12.3 12.3 12.3 12.3
12.3 12.3 12.3 12.3
12.3 12.3 12.3 12.3
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">setConstant()</span></code> can be used on any tensor where the element type
has a copy constructor and an <code class="docutils literal notranslate"><span class="pre">operator=()</span></code>:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;string, 2&gt; a(2, 3);
a.setConstant(&quot;yolo&quot;);
cout &lt;&lt; &quot;String tensor: &quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
=&gt;
String tensor:
yolo yolo yolo
yolo yolo yolo
</pre></div>
</div>
</section>
<section id="setzero">
<h3><Tensor-Type> setZero()<a class="headerlink" href="#setzero" title="Link to this heading">#</a></h3>
<p>Fills the tensor with zeros.  Equivalent to <code class="docutils literal notranslate"><span class="pre">setConstant(Scalar(0))</span></code>.
Returns the tensor itself in case you want to chain another call.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a.setZero();
cout &lt;&lt; &quot;Zeros: &quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
=&gt;
Zeros:
0 0 0 0
0 0 0 0
0 0 0 0
</pre></div>
</div>
</section>
<section id="setvalues-initializer-list">
<h3><Tensor-Type> setValues({..initializer_list})<a class="headerlink" href="#setvalues-initializer-list" title="Link to this heading">#</a></h3>
<p>Fills the tensor with explicit values specified in a std::initializer_list.
The type of the initializer list depends on the type and rank of the tensor.</p>
<p>If the tensor has rank N, the initializer list must be nested N times.  The
most deeply nested lists must contains P scalars of the Tensor type where P is
the size of the last dimension of the Tensor.</p>
<p>For example, for a <code class="docutils literal notranslate"><span class="pre">TensorFixedSize&lt;float,</span> <span class="pre">2,</span> <span class="pre">3&gt;</span></code> the initializer list must
contains 2 lists of 3 floats each.</p>
<p><code class="docutils literal notranslate"><span class="pre">setValues()</span></code> returns the tensor itself in case you want to chain another
call.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(2, 3);
a.setValues({{0.0f, 1.0f, 2.0f}, {3.0f, 4.0f, 5.0f}});
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
0 1 2
3 4 5
</pre></div>
</div>
<p>If a list is too short, the corresponding elements of the tensor will not be
changed.  This is valid at each level of nesting.  For example the following
code only sets the values of the first row of the tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setConstant(1000);
a.setValues({{10, 20, 30}});
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
10   20   30
1000 1000 1000
</pre></div>
</div>
</section>
<section id="setrandom">
<h3><Tensor-Type> setRandom()<a class="headerlink" href="#setrandom" title="Link to this heading">#</a></h3>
<p>Fills the tensor with random values.  Returns the tensor itself in case you
want to chain another call.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a.setRandom();
cout &lt;&lt; &quot;Random: &quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
=&gt;
Random:
  0.680375    0.59688  -0.329554    0.10794
 -0.211234   0.823295   0.536459 -0.0452059
  0.566198  -0.604897  -0.444451   0.257742
</pre></div>
</div>
<p>You can customize <code class="docutils literal notranslate"><span class="pre">setRandom()</span></code> by providing your own random number
generator as a template argument:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a.setRandom&lt;MyRandomGenerator&gt;();
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">MyRandomGenerator</span></code> must be a struct with the following member
functions, where Scalar and Index are the same as <code class="docutils literal notranslate"><span class="pre">&lt;Tensor-Type&gt;::``Scalar</span></code>
and <code class="docutils literal notranslate"><span class="pre">&lt;Tensor-Type&gt;::``Index</span></code>.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">struct</span> <span class="pre">UniformRandomGenerator</span></code> in TensorFunctors.h for an example.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Custom number generator for use with setRandom().
struct MyRandomGenerator {
  // Default and copy constructors. Both are needed
  MyRandomGenerator() { }
  MyRandomGenerator(const MyRandomGenerator&amp; ) { }

  // Return a random value to be used.  &quot;element_location&quot; is the
  // location of the entry to set in the tensor, it can typically
  // be ignored.
  Scalar operator()(Eigen::DenseIndex element_location,
                    Eigen::DenseIndex /*unused*/ = 0) const {
    return &lt;randomly generated value of type T&gt;;
  }

  // Same as above but generates several numbers at a time.
  typename internal::packet_traits&lt;Scalar&gt;::type packetOp(
      Eigen::DenseIndex packet_location, Eigen::DenseIndex /*unused*/ = 0) const {
    return &lt;a packet of randomly generated values&gt;;
  }
};
</pre></div>
</div>
<p>You can also use one of the 2 random number generators that are part of the
tensor library:</p>
<ul class="simple">
<li><p>UniformRandomGenerator</p></li>
<li><p>NormalRandomGenerator</p></li>
</ul>
</section>
</section>
<section id="data-access">
<h2>Data Access<a class="headerlink" href="#data-access" title="Link to this heading">#</a></h2>
<p>The Tensor, TensorFixedSize, and TensorRef classes provide the following
accessors to access the tensor coefficients:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>const Scalar&amp; operator()(const array&lt;Index, NumIndices&gt;&amp; indices)
const Scalar&amp; operator()(Index firstIndex, IndexTypes... otherIndices)
Scalar&amp; operator()(const array&lt;Index, NumIndices&gt;&amp; indices)
Scalar&amp; operator()(Index firstIndex, IndexTypes... otherIndices)
</pre></div>
</div>
<p>The number of indices must be equal to the rank of the tensor. Moreover, these
accessors are not available on tensor expressions. In order to access the
values of a tensor expression, the expression must either be evaluated or
wrapped in a TensorRef.</p>
<section id="scalar-data-and-const-scalar-data-const">
<h3>Scalar* data() and const Scalar* data() const<a class="headerlink" href="#scalar-data-and-const-scalar-data-const" title="Link to this heading">#</a></h3>
<p>Returns a pointer to the storage for the tensor.  The pointer is const if the
tensor was const.  This allows direct access to the data.  The layout of the
data depends on the tensor layout: RowMajor or ColMajor.</p>
<p>This access is usually only needed for special cases, for example when mixing
Eigen Tensor code with other libraries.</p>
<p>Scalar is the type of data stored in the tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(3, 4);
float* a_data = a.data();
a_data[0] = 123.45f;
cout &lt;&lt; &quot;a(0, 0): &quot; &lt;&lt; a(0, 0);
=&gt; a(0, 0): 123.45
</pre></div>
</div>
</section>
</section>
<section id="id2">
<h2>Tensor Operations<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>All the methods documented below return non evaluated tensor <code class="docutils literal notranslate"><span class="pre">Operations</span></code>.
These can be chained: you can apply another Tensor Operation to the value
returned by the method.</p>
<p>The chain of Operation is evaluated lazily, typically when it is assigned to a
tensor.  See “Controlling when Expression are Evaluated” for more details about
their evaluation.</p>
<section id="constant-const-scalar-val">
<h3><Operation> constant(const Scalar&amp; val)<a class="headerlink" href="#constant-const-scalar-val" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor but
where all elements have the value <code class="docutils literal notranslate"><span class="pre">val</span></code>.</p>
<p>This is useful, for example, when you want to add or subtract a constant from a
tensor, or multiply every element of a tensor by a scalar.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(2, 3);
a.setConstant(1.0f);
Eigen::Tensor&lt;float, 2&gt; b = a + a.constant(2.0f);
Eigen::Tensor&lt;float, 2&gt; c = b * b.constant(0.2f);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;c&quot; &lt;&lt; endl &lt;&lt; c &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
1 1 1
1 1 1

b
3 3 3
3 3 3

c
0.6 0.6 0.6
0.6 0.6 0.6
</pre></div>
</div>
</section>
<section id="random">
<h3><Operation> random()<a class="headerlink" href="#random" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the current tensor
but where all elements have random values.</p>
<p>This is for example useful to add random values to an existing tensor.
The generation of random values can be customized in the same manner
as for <code class="docutils literal notranslate"><span class="pre">setRandom()</span></code>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(2, 3);
a.setConstant(1.0f);
Eigen::Tensor&lt;float, 2&gt; b = a + a.random();
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
1 1 1
1 1 1

b
1.68038   1.5662  1.82329
0.788766  1.59688 0.395103
</pre></div>
</div>
</section>
</section>
<section id="unary-element-wise-operations">
<h2>Unary Element Wise Operations<a class="headerlink" href="#unary-element-wise-operations" title="Link to this heading">#</a></h2>
<p>All these operations take a single input tensor as argument and return a tensor
of the same type and dimensions as the tensor to which they are applied.  The
requested operations are applied to each element independently.</p>
<section id="operator">
<h3><Operation> operator-()<a class="headerlink" href="#operator" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the opposite values of the original tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(2, 3);
a.setConstant(1.0f);
Eigen::Tensor&lt;float, 2&gt; b = -a;
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
1 1 1
1 1 1

b
-1 -1 -1
-1 -1 -1
</pre></div>
</div>
</section>
<section id="sqrt">
<h3><Operation> sqrt()<a class="headerlink" href="#sqrt" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the square roots of the original tensor.</p>
</section>
<section id="rsqrt">
<h3><Operation> rsqrt()<a class="headerlink" href="#rsqrt" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the inverse square roots of the original tensor.</p>
</section>
<section id="square">
<h3><Operation> square()<a class="headerlink" href="#square" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the squares of the original tensor values.</p>
</section>
<section id="inverse">
<h3><Operation> inverse()<a class="headerlink" href="#inverse" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the inverse of the original tensor values.</p>
</section>
<section id="exp">
<h3><Operation> exp()<a class="headerlink" href="#exp" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the exponential of the original tensor.</p>
</section>
<section id="log">
<h3><Operation> log()<a class="headerlink" href="#log" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the natural logarithms of the original tensor.</p>
</section>
<section id="abs">
<h3><Operation> abs()<a class="headerlink" href="#abs" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the absolute values of the original tensor.</p>
</section>
<section id="pow-scalar-exponent">
<h3><Operation> pow(Scalar exponent)<a class="headerlink" href="#pow-scalar-exponent" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the original tensor
containing the coefficients of the original tensor to the power of the
exponent.</p>
<p>The type of the exponent, Scalar, is always the same as the type of the
tensor coefficients.  For example, only integer exponents can be used in
conjuntion with tensors of integer values.</p>
<p>You can use cast() to lift this restriction.  For example this computes
cubic roots of an int Tensor:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{0, 1, 8}, {27, 64, 125}});
Eigen::Tensor&lt;double, 2&gt; b = a.cast&lt;double&gt;().pow(1.0 / 3.0);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
0   1   8
27  64 125

b
0 1 2
3 4 5
</pre></div>
</div>
</section>
<section id="operator-scalar-scale">
<h3><Operation>  operator * (Scalar scale)<a class="headerlink" href="#operator-scalar-scale" title="Link to this heading">#</a></h3>
<p>Multiplies all the coefficients of the input tensor by the provided scale.</p>
</section>
<section id="cwisemax-scalar-threshold">
<h3><Operation>  cwiseMax(Scalar threshold)<a class="headerlink" href="#cwisemax-scalar-threshold" title="Link to this heading">#</a></h3>
<p>TODO</p>
</section>
<section id="cwisemin-scalar-threshold">
<h3><Operation>  cwiseMin(Scalar threshold)<a class="headerlink" href="#cwisemin-scalar-threshold" title="Link to this heading">#</a></h3>
<p>TODO</p>
</section>
<section id="unaryexpr-const-customunaryop-func">
<h3><Operation>  unaryExpr(const CustomUnaryOp&amp; func)<a class="headerlink" href="#unaryexpr-const-customunaryop-func" title="Link to this heading">#</a></h3>
<p>TODO</p>
</section>
</section>
<section id="binary-element-wise-operations">
<h2>Binary Element Wise Operations<a class="headerlink" href="#binary-element-wise-operations" title="Link to this heading">#</a></h2>
<p>These operations take two input tensors as arguments. The 2 input tensors should
be of the same type and dimensions. The result is a tensor of the same
dimensions as the tensors to which they are applied, and unless otherwise
specified it is also of the same type. The requested operations are applied to
each pair of elements independently.</p>
<section id="operator-const-otherderived-other">
<h3><Operation> operator+(const OtherDerived&amp; other)<a class="headerlink" href="#operator-const-otherderived-other" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the input tensors
containing the coefficient wise sums of the inputs.</p>
</section>
<section id="id3">
<h3><Operation> operator-(const OtherDerived&amp; other)<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the input tensors
containing the coefficient wise differences of the inputs.</p>
</section>
<section id="id4">
<h3><Operation> operator*(const OtherDerived&amp; other)<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the input tensors
containing the coefficient wise products of the inputs.</p>
</section>
<section id="id5">
<h3><Operation> operator/(const OtherDerived&amp; other)<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the input tensors
containing the coefficient wise quotients of the inputs.</p>
<p>This operator is not supported for integer types.</p>
</section>
<section id="cwisemax-const-otherderived-other">
<h3><Operation> cwiseMax(const OtherDerived&amp; other)<a class="headerlink" href="#cwisemax-const-otherderived-other" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the input tensors
containing the coefficient wise maximums of the inputs.</p>
</section>
<section id="cwisemin-const-otherderived-other">
<h3><Operation> cwiseMin(const OtherDerived&amp; other)<a class="headerlink" href="#cwisemin-const-otherderived-other" title="Link to this heading">#</a></h3>
<p>Returns a tensor of the same type and dimensions as the input tensors
containing the coefficient wise mimimums of the inputs.</p>
</section>
<section id="logical-operators">
<h3><Operation> Logical operators<a class="headerlink" href="#logical-operators" title="Link to this heading">#</a></h3>
<p>The following logical operators are supported as well:</p>
<ul class="simple">
<li><p>operator&amp;&amp;(const OtherDerived&amp; other)</p></li>
<li><p>operator||(const OtherDerived&amp; other)</p></li>
<li><p>operator&lt;(const OtherDerived&amp; other)</p></li>
<li><p>operator&lt;=(const OtherDerived&amp; other)</p></li>
<li><p>operator&gt;(const OtherDerived&amp; other)</p></li>
<li><p>operator&gt;=(const OtherDerived&amp; other)</p></li>
<li><p>operator==(const OtherDerived&amp; other)</p></li>
<li><p>operator!=(const OtherDerived&amp; other)</p></li>
</ul>
<p>They all return a tensor of boolean values.</p>
</section>
</section>
<section id="selection-select-const-thenderived-thentensor-const-elsederived-elsetensor">
<h2>Selection (select(const ThenDerived&amp; thenTensor, const ElseDerived&amp; elseTensor)<a class="headerlink" href="#selection-select-const-thenderived-thentensor-const-elsederived-elsetensor" title="Link to this heading">#</a></h2>
<p>Selection is a coefficient-wise ternary operator that is the tensor equivalent
to the if-then-else operation.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;bool, 3&gt; if = ...;
Tensor&lt;float, 3&gt; then = ...;
Tensor&lt;float, 3&gt; else = ...;
Tensor&lt;float, 3&gt; result = if.select(then, else);
</pre></div>
</div>
<p>The 3 arguments must be of the same dimensions, which will also be the dimension
of the result.  The ‘if’ tensor must be of type boolean, the ‘then’ and the
‘else’ tensor must be of the same type, which will also be the type of the
result.</p>
<p>Each coefficient in the result is equal to the corresponding coefficient in the
‘then’ tensor if the corresponding value in the ‘if’ tensor is true. If not, the
resulting coefficient will come from the ‘else’ tensor.</p>
</section>
<section id="contraction">
<h2>Contraction<a class="headerlink" href="#contraction" title="Link to this heading">#</a></h2>
<p>Tensor <em>contractions</em> are a generalization of the matrix product to the
multidimensional case.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create 2 matrices using tensors of rank 2
Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{1, 2, 3}, {6, 5, 4}});
Eigen::Tensor&lt;int, 2&gt; b(3, 2);
b.setValues({{1, 2}, {4, 5}, {5, 6}});

// Compute the traditional matrix product
Eigen::array&lt;Eigen::IndexPair&lt;int&gt;, 1&gt; product_dims = { Eigen::IndexPair&lt;int&gt;(1, 0) };
Eigen::Tensor&lt;int, 2&gt; AB = a.contract(b, product_dims);

// Compute the product of the transpose of the matrices
Eigen::array&lt;Eigen::IndexPair&lt;int&gt;, 1&gt; transposed_product_dims = { Eigen::IndexPair&lt;int&gt;(0, 1) };
Eigen::Tensor&lt;int, 2&gt; AtBt = a.contract(b, transposed_product_dims);

// Contraction to scalar value using a double contraction.
// First coordinate of both tensors are contracted as well as both second coordinates, i.e., this computes the sum of the squares of the elements.
Eigen::array&lt;Eigen::IndexPair&lt;int&gt;, 2&gt; double_contraction_product_dims = { Eigen::IndexPair&lt;int&gt;(0, 0), Eigen::IndexPair&lt;int&gt;(1, 1) };
Eigen::Tensor&lt;int, 0&gt; AdoubleContractedA = a.contract(a, double_contraction_product_dims);

// Extracting the scalar value of the tensor contraction for further usage
int value = AdoubleContractedA(0);
</pre></div>
</div>
</section>
<section id="reduction-operations">
<h2>Reduction Operations<a class="headerlink" href="#reduction-operations" title="Link to this heading">#</a></h2>
<p>A <em>Reduction</em> operation returns a tensor with fewer dimensions than the
original tensor.  The values in the returned tensor are computed by applying a
<em>reduction operator</em> to slices of values from the original tensor.  You specify
the dimensions along which the slices are made.</p>
<p>The Eigen Tensor library provides a set of predefined reduction operators such
as <code class="docutils literal notranslate"><span class="pre">maximum()</span></code> and <code class="docutils literal notranslate"><span class="pre">sum()</span></code> and lets you define additional operators by
implementing a few methods from a reductor template.</p>
<section id="reduction-dimensions">
<h3>Reduction Dimensions<a class="headerlink" href="#reduction-dimensions" title="Link to this heading">#</a></h3>
<p>All reduction operations take a single parameter of type
<code class="docutils literal notranslate"><span class="pre">&lt;TensorType&gt;::``Dimensions</span></code> which can always be specified as an array of
ints.  These are called the “reduction dimensions.”  The values are the indices
of the dimensions of the input tensor over which the reduction is done.  The
parameter can have at most as many element as the rank of the input tensor;
each element must be less than the tensor rank, as it indicates one of the
dimensions to reduce.</p>
<p>Each dimension of the input tensor should occur at most once in the reduction
dimensions as the implementation does not remove duplicates.</p>
<p>The order of the values in the reduction dimensions does not affect the
results, but the code may execute faster if you list the dimensions in
increasing order.</p>
<p>Example: Reduction along one dimension.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a tensor of 2 dimensions
Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{1, 2, 3}, {6, 5, 4}});
// Reduce it along the second dimension (1)...
Eigen::array&lt;int, 1&gt; dims({1 /* dimension to reduce */});
// ...using the &quot;maximum&quot; operator.
// The result is a tensor with one dimension.  The size of
// that dimension is the same as the first (non-reduced) dimension of a.
Eigen::Tensor&lt;int, 1&gt; b = a.maximum(dims);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
1 2 3
6 5 4

b
3
6
</pre></div>
</div>
<p>Example: Reduction along two dimensions.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 3, Eigen::ColMajor&gt; a(2, 3, 4);
a.setValues({{{0.0f, 1.0f, 2.0f, 3.0f},
              {7.0f, 6.0f, 5.0f, 4.0f},
              {8.0f, 9.0f, 10.0f, 11.0f}},
             {{12.0f, 13.0f, 14.0f, 15.0f},
              {19.0f, 18.0f, 17.0f, 16.0f},
              {20.0f, 21.0f, 22.0f, 23.0f}}});
// The tensor a has 3 dimensions.  We reduce along the
// first 2, resulting in a tensor with a single dimension
// of size 4 (the last dimension of a.)
// Note that we pass the array of reduction dimensions
// directly to the maximum() call.
Eigen::Tensor&lt;float, 1, Eigen::ColMajor&gt; b =
    a.maximum(Eigen::array&lt;int, 2&gt;({0, 1}));
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
b
20
21
22
23
</pre></div>
</div>
<section id="reduction-along-all-dimensions">
<h4>Reduction along all dimensions<a class="headerlink" href="#reduction-along-all-dimensions" title="Link to this heading">#</a></h4>
<p>As a special case, if you pass no parameter to a reduction operation the
original tensor is reduced along <em>all</em> its dimensions.  The result is a
scalar, represented as a zero-dimension tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 3&gt; a(2, 3, 4);
a.setValues({{{0.0f, 1.0f, 2.0f, 3.0f},
              {7.0f, 6.0f, 5.0f, 4.0f},
              {8.0f, 9.0f, 10.0f, 11.0f}},
             {{12.0f, 13.0f, 14.0f, 15.0f},
              {19.0f, 18.0f, 17.0f, 16.0f},
              {20.0f, 21.0f, 22.0f, 23.0f}}});
// Reduce along all dimensions using the sum() operator.
Eigen::Tensor&lt;float, 0&gt; b = a.sum();
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
b
276
</pre></div>
</div>
</section>
</section>
<section id="sum-const-dimensions-new-dims">
<h3><Operation> sum(const Dimensions&amp; new_dims)<a class="headerlink" href="#sum-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="sum">
<h3><Operation> sum()<a class="headerlink" href="#sum" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the sum() operator.  The resulting values
are the sum of the reduced values.</p>
</section>
<section id="mean-const-dimensions-new-dims">
<h3><Operation> mean(const Dimensions&amp; new_dims)<a class="headerlink" href="#mean-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="mean">
<h3><Operation> mean()<a class="headerlink" href="#mean" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the mean() operator.  The resulting values
are the mean of the reduced values.</p>
</section>
<section id="maximum-const-dimensions-new-dims">
<h3><Operation> maximum(const Dimensions&amp; new_dims)<a class="headerlink" href="#maximum-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="maximum">
<h3><Operation> maximum()<a class="headerlink" href="#maximum" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the maximum() operator.  The resulting values are the
largest of the reduced values.</p>
</section>
<section id="minimum-const-dimensions-new-dims">
<h3><Operation> minimum(const Dimensions&amp; new_dims)<a class="headerlink" href="#minimum-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="minimum">
<h3><Operation> minimum()<a class="headerlink" href="#minimum" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the minimum() operator.  The resulting values
are the smallest of the reduced values.</p>
</section>
<section id="prod-const-dimensions-new-dims">
<h3><Operation> prod(const Dimensions&amp; new_dims)<a class="headerlink" href="#prod-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="prod">
<h3><Operation> prod()<a class="headerlink" href="#prod" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the prod() operator.  The resulting values
are the product of the reduced values.</p>
</section>
<section id="all-const-dimensions-new-dims">
<h3><Operation> all(const Dimensions&amp; new_dims)<a class="headerlink" href="#all-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="all">
<h3><Operation> all()<a class="headerlink" href="#all" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the all() operator.  Casts tensor to bool and then checks
whether all elements are true.  Runs through all elements rather than
short-circuiting, so may be significantly inefficient.</p>
</section>
<section id="any-const-dimensions-new-dims">
<h3><Operation> any(const Dimensions&amp; new_dims)<a class="headerlink" href="#any-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="any">
<h3><Operation> any()<a class="headerlink" href="#any" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using the any() operator.  Casts tensor to bool and then checks
whether any element is true.  Runs through all elements rather than
short-circuiting, so may be significantly inefficient.</p>
</section>
<section id="reduce-const-dimensions-new-dims-const-reducer-reducer">
<h3><Operation> reduce(const Dimensions&amp; new_dims, const Reducer&amp; reducer)<a class="headerlink" href="#reduce-const-dimensions-new-dims-const-reducer-reducer" title="Link to this heading">#</a></h3>
<p>Reduce a tensor using a user-defined reduction operator.  See <code class="docutils literal notranslate"><span class="pre">SumReducer</span></code>
in TensorFunctors.h for information on how to implement a reduction operator.</p>
</section>
</section>
<section id="trace">
<h2>Trace<a class="headerlink" href="#trace" title="Link to this heading">#</a></h2>
<p>A <em>Trace</em> operation returns a tensor with fewer dimensions than the original
tensor. It returns a tensor whose elements are the sum of the elements of the
original tensor along the main diagonal for a list of specified dimensions, the
“trace dimensions”. Similar to the <code class="docutils literal notranslate"><span class="pre">Reduction</span> <span class="pre">Dimensions</span></code>, the trace dimensions
are passed as an input parameter to the operation, are of type <code class="docutils literal notranslate"><span class="pre">&lt;TensorType&gt;::``Dimensions</span></code>
, and have the same requirements when passed as an input parameter. In addition,
the trace dimensions must have the same size.</p>
<p>Example: Trace along 2 dimensions.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a tensor of 3 dimensions
Eigen::Tensor&lt;int, 3&gt; a(2, 2, 3);
a.setValues({{{1, 2, 3}, {4, 5, 6}}, {{7, 8, 9}, {10, 11, 12}}});
// Specify the dimensions along which the trace will be computed.
// In this example, the trace can only be computed along the dimensions
// with indices 0 and 1
Eigen::array&lt;int, 2&gt; dims({0, 1});
// The output tensor contains all but the trace dimensions.
Tensor&lt;int, 1&gt; a_trace = a.trace(dims);
cout &lt;&lt; &quot;a_trace:&quot; &lt;&lt; endl;
cout &lt;&lt; a_trace &lt;&lt; endl;
=&gt;
a_trace:
11
13
15
</pre></div>
</div>
<section id="trace-const-dimensions-new-dims">
<h3><Operation> trace(const Dimensions&amp; new_dims)<a class="headerlink" href="#trace-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
</section>
<section id="id6">
<h3><Operation> trace()<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>As a special case, if no parameter is passed to the operation, trace is computed
along <em>all</em> dimensions of the input tensor.</p>
<p>Example: Trace along all dimensions.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a tensor of 3 dimensions, with all dimensions having the same size.
Eigen::Tensor&lt;int, 3&gt; a(3, 3, 3);
a.setValues({{{1, 2, 3}, {4, 5, 6}, {7, 8, 9}},
            {{10, 11, 12}, {13, 14, 15}, {16, 17, 18}},
            {{19, 20, 21}, {22, 23, 24}, {25, 26, 27}}});
// Result is a zero dimension tensor
Tensor&lt;int, 0&gt; a_trace = a.trace();
cout&lt;&lt;&quot;a_trace:&quot;&lt;&lt;endl;
cout&lt;&lt;a_trace&lt;&lt;endl;
=&gt;
a_trace:
42
</pre></div>
</div>
</section>
</section>
<section id="scan-operations">
<h2>Scan Operations<a class="headerlink" href="#scan-operations" title="Link to this heading">#</a></h2>
<p>A <em>Scan</em> operation returns a tensor with the same dimensions as the original
tensor. The operation performs an inclusive scan along the specified
axis, which means it computes a running total along the axis for a given
reduction operation.
If the reduction operation corresponds to summation, then this computes the
prefix sum of the tensor along the given axis.</p>
<p>Example:
dd a comment to this line</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Create a tensor of 2 dimensions
Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{1, 2, 3}, {4, 5, 6}});
// Scan it along the second dimension (1) using summation
Eigen::Tensor&lt;int, 2&gt; b = a.cumsum(1);
// The result is a tensor with the same size as the input
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
1 2 3
4 5 6

b
1  3  6
4  9 15
</pre></div>
</div>
<section id="cumsum-const-index-axis">
<h3><Operation> cumsum(const Index&amp; axis)<a class="headerlink" href="#cumsum-const-index-axis" title="Link to this heading">#</a></h3>
<p>Perform a scan by summing consecutive entries.</p>
</section>
<section id="cumprod-const-index-axis">
<h3><Operation> cumprod(const Index&amp; axis)<a class="headerlink" href="#cumprod-const-index-axis" title="Link to this heading">#</a></h3>
<p>Perform a scan by multiplying consecutive entries.</p>
</section>
</section>
<section id="convolutions">
<h2>Convolutions<a class="headerlink" href="#convolutions" title="Link to this heading">#</a></h2>
<section id="convolve-const-kernel-kernel-const-dimensions-dims">
<h3><Operation> convolve(const Kernel&amp; kernel, const Dimensions&amp; dims)<a class="headerlink" href="#convolve-const-kernel-kernel-const-dimensions-dims" title="Link to this heading">#</a></h3>
<p>Returns a tensor that is the output of the convolution of the input tensor with the kernel,
along the specified dimensions of the input tensor. The dimension size for dimensions of the output tensor
which were part of the convolution will be reduced by the formula:
output_dim_size = input_dim_size - kernel_dim_size + 1 (requires: input_dim_size &gt;= kernel_dim_size).
The dimension sizes for dimensions that were not part of the convolution will remain the same.
Performance of the convolution can depend on the length of the stride(s) of the input tensor dimension(s) along which the
convolution is computed (the first dimension has the shortest stride for ColMajor, whereas RowMajor’s shortest stride is
for the last dimension).</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Compute convolution along the second and third dimension.
Tensor&lt;float, 4, DataLayout&gt; input(3, 3, 7, 11);
Tensor&lt;float, 2, DataLayout&gt; kernel(2, 2);
Tensor&lt;float, 4, DataLayout&gt; output(3, 2, 6, 11);
input.setRandom();
kernel.setRandom();

Eigen::array&lt;ptrdiff_t, 2&gt; dims({1, 2});  // Specify second and third dimension for convolution.
output = input.convolve(kernel, dims);

for (int i = 0; i &lt; 3; ++i) {
  for (int j = 0; j &lt; 2; ++j) {
    for (int k = 0; k &lt; 6; ++k) {
      for (int l = 0; l &lt; 11; ++l) {
        const float result = output(i,j,k,l);
        const float expected = input(i,j+0,k+0,l) * kernel(0,0) +
                               input(i,j+1,k+0,l) * kernel(1,0) +
                               input(i,j+0,k+1,l) * kernel(0,1) +
                               input(i,j+1,k+1,l) * kernel(1,1);
        VERIFY_IS_APPROX(result, expected);
      }
    }
  }
}
</pre></div>
</div>
</section>
</section>
<section id="geometrical-operations">
<h2>Geometrical Operations<a class="headerlink" href="#geometrical-operations" title="Link to this heading">#</a></h2>
<p>These operations return a Tensor with different dimensions than the original
Tensor.  They can be used to access slices of tensors, see them with different
dimensions, or pad tensors with additional data.</p>
<section id="reshape-const-dimensions-new-dims">
<h3><Operation> reshape(const Dimensions&amp; new_dims)<a class="headerlink" href="#reshape-const-dimensions-new-dims" title="Link to this heading">#</a></h3>
<p>Returns a view of the input tensor that has been reshaped to the specified
new dimensions.  The argument new_dims is an array of Index values.  The
rank of the resulting tensor is equal to the number of elements in new_dims.</p>
<p>The product of all the sizes in the new dimension array must be equal to
the number of elements in the input tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Increase the rank of the input tensor by introducing a new dimension
// of size 1.
Tensor&lt;float, 2&gt; input(7, 11);
array&lt;int, 3&gt; three_dims{{7, 11, 1}};
Tensor&lt;float, 3&gt; result = input.reshape(three_dims);

// Decrease the rank of the input tensor by merging 2 dimensions;
array&lt;int, 1&gt; one_dim{{7 * 11}};
Tensor&lt;float, 1&gt; result = input.reshape(one_dim);
</pre></div>
</div>
<p>This operation does not move any data in the input tensor, so the resulting
contents of a reshaped Tensor depend on the data layout of the original Tensor.</p>
<p>For example this is what happens when you <code class="docutils literal notranslate"><span class="pre">reshape()</span></code> a 2D ColMajor tensor
to one dimension:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2, Eigen::ColMajor&gt; a(2, 3);
a.setValues({{0.0f, 100.0f, 200.0f}, {300.0f, 400.0f, 500.0f}});
Eigen::array&lt;Eigen::DenseIndex, 1&gt; one_dim({3 * 2});
Eigen::Tensor&lt;float, 1, Eigen::ColMajor&gt; b = a.reshape(one_dim);
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
b
  0
300
100
400
200
500
</pre></div>
</div>
<p>This is what happens when the 2D Tensor is RowMajor:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2, Eigen::RowMajor&gt; a(2, 3);
a.setValues({{0.0f, 100.0f, 200.0f}, {300.0f, 400.0f, 500.0f}});
Eigen::array&lt;Eigen::DenseIndex, 1&gt; one_dim({3 * 2});
Eigen::Tensor&lt;float, 1, Eigen::RowMajor&gt; b = a.reshape(one_dim);
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
b
  0
100
200
300
400
500
</pre></div>
</div>
<p>The reshape operation is a lvalue. In other words, it can be used on the left
side of the assignment operator.</p>
<p>The previous example can be rewritten as follow:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2, Eigen::ColMajor&gt; a(2, 3);
a.setValues({{0.0f, 100.0f, 200.0f}, {300.0f, 400.0f, 500.0f}});
Eigen::array&lt;Eigen::DenseIndex, 2&gt; two_dim({2, 3});
Eigen::Tensor&lt;float, 1, Eigen::ColMajor&gt; b(6);
b.reshape(two_dim) = a;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
b
  0
300
100
400
200
500
</pre></div>
</div>
<p>Note that “b” itself was not reshaped but that instead the assignment is done to
the reshape view of b.</p>
</section>
<section id="shuffle-const-shuffle-shuffle">
<h3><Operation> shuffle(const Shuffle&amp; shuffle)<a class="headerlink" href="#shuffle-const-shuffle-shuffle" title="Link to this heading">#</a></h3>
<p>Returns a copy of the input tensor whose dimensions have been
reordered according to the specified permutation. The argument shuffle
is an array of Index values. Its size is the rank of the input
tensor. It must contain a permutation of 0, 1, …, rank - 1. The i-th
dimension of the output tensor equals to the size of the shuffle[i]-th
dimension of the input tensor. For example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Shuffle all dimensions to the left by 1.
Tensor&lt;float, 3&gt; input(20, 30, 50);
// ... set some values in input.
Tensor&lt;float, 3&gt; output = input.shuffle({1, 2, 0})

eigen_assert(output.dimension(0) == 30);
eigen_assert(output.dimension(1) == 50);
eigen_assert(output.dimension(2) == 20);
</pre></div>
</div>
<p>Indices into the output tensor are shuffled accordingly to formulate
indices into the input tensor. For example, one can assert in the above
code snippet that:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>eigen_assert(output(3, 7, 11) == input(11, 3, 7));
</pre></div>
</div>
<p>In general, one can assert that</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>eigen_assert(output(..., indices[shuffle[i]], ...) ==
             input(..., indices[i], ...))
</pre></div>
</div>
<p>The shuffle operation results in a lvalue, which means that it can be assigned
to. In other words, it can be used on the left side of the assignment operator.</p>
<p>Let’s rewrite the previous example to take advantage of this feature:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>// Shuffle all dimensions to the left by 1.
Tensor&lt;float, 3&gt; input(20, 30, 50);
// ... set some values in input.
Tensor&lt;float, 3&gt; output(30, 50, 20);
output.shuffle({2, 0, 1}) = input;
</pre></div>
</div>
</section>
<section id="stride-const-strides-strides">
<h3><Operation> stride(const Strides&amp; strides)<a class="headerlink" href="#stride-const-strides-strides" title="Link to this heading">#</a></h3>
<p>Returns a view of the input tensor that strides (skips stride-1
elements) along each of the dimensions.  The argument strides is an
array of Index values.  The dimensions of the resulting tensor are
ceil(input_dimensions[i] / strides[i]).</p>
<p>For example this is what happens when you <code class="docutils literal notranslate"><span class="pre">stride()</span></code> a 2D tensor:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(4, 3);
a.setValues({{0, 100, 200}, {300, 400, 500}, {600, 700, 800}, {900, 1000, 1100}});
Eigen::array&lt;Eigen::DenseIndex, 2&gt; strides({3, 2});
Eigen::Tensor&lt;int, 2&gt; b = a.stride(strides);
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
b
   0   200
 900  1100
</pre></div>
</div>
<p>It is possible to assign a tensor to a stride:
Tensor&lt;float, 3&gt; input(20, 30, 50);
// … set some values in input.
Tensor&lt;float, 3&gt; output(40, 90, 200);
output.stride({2, 3, 4}) = input;</p>
</section>
<section id="slice-const-startindices-offsets-const-sizes-extents">
<h3><Operation> slice(const StartIndices&amp; offsets, const Sizes&amp; extents)<a class="headerlink" href="#slice-const-startindices-offsets-const-sizes-extents" title="Link to this heading">#</a></h3>
<p>Returns a sub-tensor of the given tensor. For each dimension i, the slice is
made of the coefficients stored between offset[i] and offset[i] + extents[i] in
the input tensor.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(4, 3);
a.setValues({{0, 100, 200}, {300, 400, 500},
             {600, 700, 800}, {900, 1000, 1100}});
Eigen::array&lt;int, 2&gt; offsets = {1, 0};
Eigen::array&lt;int, 2&gt; extents = {2, 2};
Eigen::Tensor&lt;int, 1&gt; slice = a.slice(offsets, extents);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl;
=&gt;
a
   0   100   200
 300   400   500
 600   700   800
 900  1000  1100
cout &lt;&lt; &quot;slice&quot; &lt;&lt; endl &lt;&lt; slice &lt;&lt; endl;
=&gt;
slice
 300   400
 600   700
</pre></div>
</div>
</section>
<section id="chip-const-index-offset-const-index-dim">
<h3><Operation> chip(const Index offset, const Index dim)<a class="headerlink" href="#chip-const-index-offset-const-index-dim" title="Link to this heading">#</a></h3>
<p>A chip is a special kind of slice. It is the subtensor at the given offset in
the dimension dim. The returned tensor has one fewer dimension than the input
tensor: the dimension dim is removed.</p>
<p>For example, a matrix chip would be either a row or a column of the input
matrix.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(4, 3);
a.setValues({{0, 100, 200}, {300, 400, 500},
             {600, 700, 800}, {900, 1000, 1100}});
Eigen::Tensor&lt;int, 1&gt; row_3 = a.chip(2, 0);
Eigen::Tensor&lt;int, 1&gt; col_2 = a.chip(1, 1);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl;
=&gt;
a
   0   100   200
 300   400   500
 600   700   800
 900  1000  1100
cout &lt;&lt; &quot;row_3&quot; &lt;&lt; endl &lt;&lt; row_3 &lt;&lt; endl;
=&gt;
row_3
   600   700   800
cout &lt;&lt; &quot;col_2&quot; &lt;&lt; endl &lt;&lt; col_2 &lt;&lt; endl;
=&gt;
col_2
   100   400   700    1000
</pre></div>
</div>
<p>It is possible to assign values to a tensor chip since the chip operation is a
lvalue. For example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 1&gt; a(3);
a.setValues({{100, 200, 300}});
Eigen::Tensor&lt;int, 2&gt; b(2, 3);
b.setZero();
b.chip(0, 0) = a;
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl;
=&gt;
a
 100
 200
 300
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
b
   100   200   300
     0     0     0
</pre></div>
</div>
</section>
<section id="reverse-const-reversedimensions-reverse">
<h3><Operation> reverse(const ReverseDimensions&amp; reverse)<a class="headerlink" href="#reverse-const-reversedimensions-reverse" title="Link to this heading">#</a></h3>
<p>Returns a view of the input tensor that reverses the order of the coefficients
along a subset of the dimensions.  The argument reverse is an array of boolean
values that indicates whether or not the order of the coefficients should be
reversed along each of the dimensions.  This operation preserves the dimensions
of the input tensor.</p>
<p>For example this is what happens when you <code class="docutils literal notranslate"><span class="pre">reverse()</span></code> the first dimension
of a 2D tensor:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(4, 3);
a.setValues({{0, 100, 200}, {300, 400, 500},
            {600, 700, 800}, {900, 1000, 1100}});
Eigen::array&lt;bool, 2&gt; reverse({true, false});
Eigen::Tensor&lt;int, 2&gt; b = a.reverse(reverse);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
a
   0   100   200
 300   400   500
 600   700   800
 900  1000  1100
b
 900  1000  1100
 600   700   800
 300   400   500
   0   100   200
</pre></div>
</div>
</section>
<section id="broadcast-const-broadcast-broadcast">
<h3><Operation> broadcast(const Broadcast&amp; broadcast)<a class="headerlink" href="#broadcast-const-broadcast-broadcast" title="Link to this heading">#</a></h3>
<p>Returns a view of the input tensor in which the input is replicated one to many
times.
The broadcast argument specifies how many copies of the input tensor need to be
made in each of the dimensions.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{0, 100, 200}, {300, 400, 500}});
Eigen::array&lt;int, 2&gt; bcast({3, 2});
Eigen::Tensor&lt;int, 2&gt; b = a.broadcast(bcast);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
a
   0   100   200
 300   400   500
b
   0   100   200    0   100   200
 300   400   500  300   400   500
   0   100   200    0   100   200
 300   400   500  300   400   500
   0   100   200    0   100   200
 300   400   500  300   400   500
</pre></div>
</div>
</section>
<section id="concatenate-const-otherderived-other-axis-axis">
<h3><Operation> concatenate(const OtherDerived&amp; other, Axis axis)<a class="headerlink" href="#concatenate-const-otherderived-other-axis-axis" title="Link to this heading">#</a></h3>
<p>TODO</p>
</section>
<section id="pad-const-paddingdimensions-padding">
<h3><Operation>  pad(const PaddingDimensions&amp; padding)<a class="headerlink" href="#pad-const-paddingdimensions-padding" title="Link to this heading">#</a></h3>
<p>Returns a view of the input tensor in which the input is padded with zeros.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{0, 100, 200}, {300, 400, 500}});
Eigen::array&lt;pair&lt;int, int&gt;, 2&gt; paddings;
paddings[0] = make_pair(0, 1);
paddings[1] = make_pair(2, 3);
Eigen::Tensor&lt;int, 2&gt; b = a.pad(paddings);
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl;
=&gt;
a
   0   100   200
 300   400   500
b
   0     0     0    0
   0     0     0    0
   0   100   200    0
 300   400   500    0
   0     0     0    0
   0     0     0    0
   0     0     0    0
</pre></div>
</div>
</section>
<section id="extract-patches-const-patchdims-patch-dims">
<h3><Operation>  extract_patches(const PatchDims&amp; patch_dims)<a class="headerlink" href="#extract-patches-const-patchdims-patch-dims" title="Link to this heading">#</a></h3>
<p>Returns a tensor of coefficient patches extracted from the input tensor, where
each patch is of dimension specified by ‘patch_dims’. The returned tensor has
one greater dimension than the input tensor, which is used to index each patch.
The patch index in the output tensor depends on the data layout of the input
tensor: the patch index is the last dimension ColMajor layout, and the first
dimension in RowMajor layout.</p>
<p>For example, given the following input tensor:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2, DataLayout&gt; tensor(3,4);
tensor.setValues({{0.0f, 1.0f, 2.0f, 3.0f},
                  {4.0f, 5.0f, 6.0f, 7.0f},
                  {8.0f, 9.0f, 10.0f, 11.0f}});

cout &lt;&lt; &quot;tensor: &quot; &lt;&lt; endl &lt;&lt; tensor &lt;&lt; endl;
=&gt;
tensor:
 0   1   2   3
 4   5   6   7
 8   9  10  11
</pre></div>
</div>
<p>Six 2x2 patches can be extracted and indexed using the following code:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 3, DataLayout&gt; patch;
Eigen::array&lt;ptrdiff_t, 2&gt; patch_dims;
patch_dims[0] = 2;
patch_dims[1] = 2;
patch = tensor.extract_patches(patch_dims);
for (int k = 0; k &lt; 6; ++k) {
  cout &lt;&lt; &quot;patch index: &quot; &lt;&lt; k &lt;&lt; endl;
  for (int i = 0; i &lt; 2; ++i) {
	for (int j = 0; j &lt; 2; ++j) {
	  if (DataLayout == ColMajor) {
		cout &lt;&lt; patch(i, j, k) &lt;&lt; &quot; &quot;;
	  } else {
		cout &lt;&lt; patch(k, i, j) &lt;&lt; &quot; &quot;;
	  }
	}
	cout &lt;&lt; endl;
  }
}
</pre></div>
</div>
<p>This code results in the following output when the data layout is ColMajor:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>patch index: 0
0 1
4 5
patch index: 1
4 5
8 9
patch index: 2
1 2
5 6
patch index: 3
5 6
9 10
patch index: 4
2 3
6 7
patch index: 5
6 7
10 11
</pre></div>
</div>
<p>This code results in the following output when the data layout is RowMajor:
(NOTE: the set of patches is the same as in ColMajor, but are indexed differently).</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>patch index: 0
0 1
4 5
patch index: 1
1 2
5 6
patch index: 2
2 3
6 7
patch index: 3
4 5
8 9
patch index: 4
5 6
9 10
patch index: 5
6 7
10 11
</pre></div>
</div>
</section>
<section id="extract-image-patches-const-index-patch-rows-const-index-patch-cols-const-index-row-stride-const-index-col-stride-const-paddingtype-padding-type">
<h3><Operation>  extract_image_patches(const Index patch_rows, const Index patch_cols, const Index row_stride, const Index col_stride, const PaddingType padding_type)<a class="headerlink" href="#extract-image-patches-const-index-patch-rows-const-index-patch-cols-const-index-row-stride-const-index-col-stride-const-paddingtype-padding-type" title="Link to this heading">#</a></h3>
<p>Returns a tensor of coefficient image patches extracted from the input tensor,
which is expected to have dimensions ordered as follows (depending on the data
layout of the input tensor, and the number of additional dimensions ‘N’):</p>
<p>*) ColMajor
1st dimension: channels (of size d)
2nd dimension: rows (of size r)
3rd dimension: columns (of size c)
4th-Nth dimension: time (for video) or batch (for bulk processing).</p>
<p>*) RowMajor (reverse order of ColMajor)
1st-Nth dimension: time (for video) or batch (for bulk processing).
N+1’th dimension: columns (of size c)
N+2’th dimension: rows (of size r)
N+3’th dimension: channels (of size d)</p>
<p>The returned tensor has one greater dimension than the input tensor, which is
used to index each patch. The patch index in the output tensor depends on the
data layout of the input tensor: the patch index is the 4’th dimension in
ColMajor layout, and the 4’th from the last dimension in RowMajor layout.</p>
<p>For example, given the following input tensor with the following dimension
sizes:
*) depth:   2
*) rows:    3
*) columns: 5
*) batch:   7</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 4&gt; tensor(2,3,5,7);
Tensor&lt;float, 4, RowMajor&gt; tensor_row_major = tensor.swap_layout();
</pre></div>
</div>
<p>2x2 image patches can be extracted and indexed using the following code:</p>
<p>*) 2D patch: ColMajor (patch indexed by second-to-last dimension)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 5&gt; twod_patch;
twod_patch = tensor.extract_image_patches&lt;2, 2&gt;();
// twod_patch.dimension(0) == 2
// twod_patch.dimension(1) == 2
// twod_patch.dimension(2) == 2
// twod_patch.dimension(3) == 3*5
// twod_patch.dimension(4) == 7
</pre></div>
</div>
<p>*) 2D patch: RowMajor (patch indexed by the second dimension)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Tensor&lt;float, 5, RowMajor&gt; twod_patch_row_major;
twod_patch_row_major = tensor_row_major.extract_image_patches&lt;2, 2&gt;();
// twod_patch_row_major.dimension(0) == 7
// twod_patch_row_major.dimension(1) == 3*5
// twod_patch_row_major.dimension(2) == 2
// twod_patch_row_major.dimension(3) == 2
// twod_patch_row_major.dimension(4) == 2
</pre></div>
</div>
</section>
</section>
<section id="special-operations">
<h2>Special Operations<a class="headerlink" href="#special-operations" title="Link to this heading">#</a></h2>
<section id="cast">
<h3><Operation> cast<T>()<a class="headerlink" href="#cast" title="Link to this heading">#</a></h3>
<p>Returns a tensor of type T with the same dimensions as the original tensor.
The returned tensor contains the values of the original tensor converted to
type T.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;float, 2&gt; a(2, 3);
Eigen::Tensor&lt;int, 2&gt; b = a.cast&lt;int&gt;();
</pre></div>
</div>
<p>This can be useful for example if you need to do element-wise division of
Tensors of integers.  This is not currently supported by the Tensor library
but you can easily cast the tensors to floats to do the division:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Eigen::Tensor&lt;int, 2&gt; a(2, 3);
a.setValues({{0, 1, 2}, {3, 4, 5}});
Eigen::Tensor&lt;int, 2&gt; b =
    (a.cast&lt;float&gt;() / a.constant(2).cast&lt;float&gt;()).cast&lt;int&gt;();
cout &lt;&lt; &quot;a&quot; &lt;&lt; endl &lt;&lt; a &lt;&lt; endl &lt;&lt; endl;
cout &lt;&lt; &quot;b&quot; &lt;&lt; endl &lt;&lt; b &lt;&lt; endl &lt;&lt; endl;
=&gt;
a
0 1 2
3 4 5

b
0 0 1
1 2 2
</pre></div>
</div>
</section>
<section id="eval">
<h3><Operation>     eval()<a class="headerlink" href="#eval" title="Link to this heading">#</a></h3>
<p>TODO</p>
</section>
</section>
<section id="representation-of-scalar-values">
<h2>Representation of scalar values<a class="headerlink" href="#representation-of-scalar-values" title="Link to this heading">#</a></h2>
<p>Scalar values are often represented by tensors of size 1 and rank 0.For example
Tensor&lt;T, N&gt;::maximum() currently returns a Tensor&lt;T, 0&gt;. Similarly, the inner
product of 2 1d tensors (through contractions) returns a 0d tensor.</p>
</section>
<section id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The number of tensor dimensions is currently limited to 250 when using a
compiler that supports cxx11. It is limited to only 5 for older compilers.</p></li>
<li><p>The IndexList class requires a cxx11 compliant compiler. You can use an
array of indices instead if you don’t have access to a modern compiler.</p></li>
<li><p>On GPUs only floating point values are properly tested and optimized for.</p></li>
<li><p>Complex and integer values are known to be broken on GPUs. If you try to use
them you’ll most likely end up triggering a static assertion failure such as
EIGEN_STATIC_ASSERT(packetSize &gt; 1, YOU_MADE_A_PROGRAMMING_MISTAKE)</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./renv\library\windows\R-4.4\x86_64-w64-mingw32\RcppEigen\include\unsupported\Eigen\CXX11\src\Tensor"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-classes">Tensor Classes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensor-data-type-rank">Class Tensor&lt;data_type, rank&gt;</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor-tensor-data-type-rank-size0-size1">Constructor Tensor&lt;data_type, rank&gt;(size0, size1, …)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor-tensor-data-type-rank-size-array">Constructor Tensor&lt;data_type, rank&gt;(size_array)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensorfixedsize-data-type-sizes-size0-size1">Class TensorFixedSize&lt;data_type, Sizes&lt;size0, size1, …&gt;&gt;</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensormap-tensor-data-type-rank">Class TensorMap&lt;Tensor&lt;data_type, rank&gt;&gt;</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor-tensormap-tensor-data-type-rank-data-size0-size1">Constructor TensorMap&lt;Tensor&lt;data_type, rank&gt;&gt;(data, size0, size1, …)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#class-tensorref">Class TensorRef</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-tensor-elements">Accessing Tensor Elements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-type-tensor-index0-index1">&lt;data_type&gt; tensor(index0, index1…)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorlayout">TensorLayout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-operations">Tensor Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-operations-and-c-auto">Tensor Operations and C++ “auto”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-when-expression-are-evaluated">Controlling When Expression are Evaluated</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#assigning-to-a-tensor-tensorfixedsize-or-tensormap">Assigning to a Tensor, TensorFixedSize, or TensorMap.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calling-eval">Calling eval().</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#assigning-to-a-tensorref">Assigning to a TensorRef.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-how-expressions-are-evaluated">Controlling How Expressions Are Evaluated</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-with-the-defaultdevice">Evaluating With the DefaultDevice</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-with-a-thread-pool">Evaluating with a Thread Pool</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-on-gpu">Evaluating On GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datatypes">Datatypes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions"><tensor-type>::Dimensions</tensor-type></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#index"><tensor-type>::Index</tensor-type></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar"><tensor-type>::Scalar</tensor-type></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><operation></operation></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#built-in-tensor-methods">Built-in Tensor Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metadata">Metadata</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#int-numdimensions">int NumDimensions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions-dimensions">Dimensions dimensions()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-dimension-index-n">Index dimension(Index n)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-size">Index size()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-dimensions-from-an-operation">Getting Dimensions From An Operation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constructors">Constructors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">Tensor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorfixedsize">TensorFixedSize</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensormap">TensorMap</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents-initialization">Contents Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setconstant-const-scalar-val"><tensor-type> setConstant(const Scalar&amp; val)</tensor-type></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setzero"><tensor-type> setZero()</tensor-type></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setvalues-initializer-list"><tensor-type> setValues({..initializer_list})</tensor-type></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setrandom"><tensor-type> setRandom()</tensor-type></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-access">Data Access</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scalar-data-and-const-scalar-data-const">Scalar* data() and const Scalar* data() const</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Tensor Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constant-const-scalar-val"><operation> constant(const Scalar&amp; val)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random"><operation> random()</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unary-element-wise-operations">Unary Element Wise Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator"><operation> operator-()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sqrt"><operation> sqrt()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rsqrt"><operation> rsqrt()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#square"><operation> square()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse"><operation> inverse()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exp"><operation> exp()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#log"><operation> log()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#abs"><operation> abs()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pow-scalar-exponent"><operation> pow(Scalar exponent)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-scalar-scale"><operation>  operator * (Scalar scale)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemax-scalar-threshold"><operation>  cwiseMax(Scalar threshold)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemin-scalar-threshold"><operation>  cwiseMin(Scalar threshold)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unaryexpr-const-customunaryop-func"><operation>  unaryExpr(const CustomUnaryOp&amp; func)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-element-wise-operations">Binary Element Wise Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-const-otherderived-other"><operation> operator+(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><operation> operator-(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><operation> operator*(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><operation> operator/(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemax-const-otherderived-other"><operation> cwiseMax(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cwisemin-const-otherderived-other"><operation> cwiseMin(const OtherDerived&amp; other)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-operators"><operation> Logical operators</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-select-const-thenderived-thentensor-const-elsederived-elsetensor">Selection (select(const ThenDerived&amp; thenTensor, const ElseDerived&amp; elseTensor)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contraction">Contraction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-operations">Reduction Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-dimensions">Reduction Dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-along-all-dimensions">Reduction along all dimensions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-const-dimensions-new-dims"><operation> sum(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum"><operation> sum()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-const-dimensions-new-dims"><operation> mean(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean"><operation> mean()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-const-dimensions-new-dims"><operation> maximum(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum"><operation> maximum()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum-const-dimensions-new-dims"><operation> minimum(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#minimum"><operation> minimum()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prod-const-dimensions-new-dims"><operation> prod(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prod"><operation> prod()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#all-const-dimensions-new-dims"><operation> all(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#all"><operation> all()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#any-const-dimensions-new-dims"><operation> any(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#any"><operation> any()</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-const-dimensions-new-dims-const-reducer-reducer"><operation> reduce(const Dimensions&amp; new_dims, const Reducer&amp; reducer)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trace">Trace</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trace-const-dimensions-new-dims"><operation> trace(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><operation> trace()</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scan-operations">Scan Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumsum-const-index-axis"><operation> cumsum(const Index&amp; axis)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cumprod-const-index-axis"><operation> cumprod(const Index&amp; axis)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutions">Convolutions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolve-const-kernel-kernel-const-dimensions-dims"><operation> convolve(const Kernel&amp; kernel, const Dimensions&amp; dims)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geometrical-operations">Geometrical Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshape-const-dimensions-new-dims"><operation> reshape(const Dimensions&amp; new_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffle-const-shuffle-shuffle"><operation> shuffle(const Shuffle&amp; shuffle)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride-const-strides-strides"><operation> stride(const Strides&amp; strides)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#slice-const-startindices-offsets-const-sizes-extents"><operation> slice(const StartIndices&amp; offsets, const Sizes&amp; extents)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chip-const-index-offset-const-index-dim"><operation> chip(const Index offset, const Index dim)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-const-reversedimensions-reverse"><operation> reverse(const ReverseDimensions&amp; reverse)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcast-const-broadcast-broadcast"><operation> broadcast(const Broadcast&amp; broadcast)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#concatenate-const-otherderived-other-axis-axis"><operation> concatenate(const OtherDerived&amp; other, Axis axis)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pad-const-paddingdimensions-padding"><operation>  pad(const PaddingDimensions&amp; padding)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-patches-const-patchdims-patch-dims"><operation>  extract_patches(const PatchDims&amp; patch_dims)</operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-image-patches-const-index-patch-rows-const-index-patch-cols-const-index-row-stride-const-index-col-stride-const-paddingtype-padding-type"><operation>  extract_image_patches(const Index patch_rows, const Index patch_cols, const Index row_stride, const Index col_stride, const PaddingType padding_type)</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-operations">Special Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cast"><operation> cast<t>()</t></operation></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eval"><operation>     eval()</operation></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representation-of-scalar-values">Representation of scalar values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Brittany Cummings
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../../../../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../../../../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>